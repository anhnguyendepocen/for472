[
["descriptive-statistics-point-estimators-and-interval-estimators.html", "Chapter 9 Descriptive Statistics, Point Estimators, and Interval Estimators 9.1 Descriptive Statistics 101 9.2 Measures of Central Tendency 9.3 Measures of Dispersion 9.4 Bias, Accuracy, and Precision 9.5 Sumarizing Uncertainty", " Chapter 9 Descriptive Statistics, Point Estimators, and Interval Estimators As researchers in the forest and environmental sciences, perhaps the most important thing we do is organize, calculate measures, and present data in a way that makes it easy to understand the information contained in the data. Although this may initially sound simple, there are an immense number of ways to summarize different types of data. Our task is to identify the data type and choose the most appropriate summary measure and/or graphical display for each specific goal we seek to accomplish. In this chapter, we will learn the basic tools necessary to accomplish these tasks. Specifically, we will present a brief overview of possible descriptive statistics, point estimators, and interval estimators to describe a data set, as well as learn the necessary code in R to obtain these estimators and produce basic plots of our data. 9.1 Descriptive Statistics 101 Two central concepts to any ecological data set are the population and sample. A Population is a complete set of items that share at least one property in common that is the subject of a statistical analysis. The population is the unit on which we seek inference. If we want to determine the mean DBH of trees in East Lansing, the population is all trees in East Lansing. If we want to determine the mean volume of oak trees in a national park, the population consists of all oak trees in the national park. As is clear from these two examples, it is often not reasonable to obtain a measurement for the entire population to answer our question of interest. One of the primary goals of statistics is to use a sample to draw inference about an entire population. A Sample is a subset drawn from the population to represent the population in a statistical analysis. If the sample is selected properly, we can use it to draw inference about the population. We will learn about useful sample schemes in great depth in Chapters cite chapters here, but for now we move forward with our initial exploration of descriptive statistics. When a sample of data is collected, there is variabiltiy in the different observations. When measuring DBH of trees in a forest stand, there is large variability in the measurements from individual to individual as a result of the species of the indvidual, location, age, measurement error, etc. This creates a distribution of measurements that we can visualize to obtain information regarding our collected sample. One simple and useful way to visualize a data distribution is to use a Histogram. Figure 9.1 displays three very different histograms. A Symmetric distribution is a distribution such that the vertical axis divides the distribution into two identical parts. A Skewed distribution is Asymmetric (cannot be divided into two equal parts along the vertical axis). A Positive Skewed distribution is a distribution that has a long tail to the right, while a Negative Skewed distribution has a long tail to the left. Looking at the data points themselves, it is quite difficult to determine whether or not the data are symmetric or skewed, but it is clear from Figure 9.1 that creating simple histograms to visualize the data makes it very easy to determine the skewness of the data. FIGURE 9.1: Symmetric and asymmetric distributions of simulated data 9.2 Measures of Central Tendency Visualizing a data set using histograms is a great way to get an idea of the data distribution and common values in the data set, but we typically want a quantitative measure to describe the central tendency, or a typical value, of a data set. We will explore four measures of central tendency: Arithmetic Mean Quadratic Mean Median Mode 9.2.1 Arithmetic Mean The arithmetic mean (often called simply the mean) is the unweighted average of a set of data points. There are two arithmetic means of interest to us: Population \\[\\begin{equation} \\mu = \\frac{\\sum_{i = 1}^Nx_i}{N} \\tag{9.1} \\end{equation}\\] where \\(N\\) is the total number of units in the population, and \\(x_i\\) for \\(i = 1, 2, \\dots N\\) are the data points. Sample \\[\\begin{equation} \\bar{x} = \\frac{\\sum_{i = 1}^nx_i}{n} \\tag{9.2} \\end{equation}\\] where \\(n\\) is the number of sample units. The arithmetic mean is a very common summary measure of a data set. However, the arithmetic mean is very sensitive to extreme values and hence it is not always the best summary of central tendency. 9.2.2 Quadratic Mean An alternative measure of central tendency is the quadratic mean. Again, we distinguish between the population and sample quadratic means: Population \\[\\begin{equation} \\mu_Q = \\sqrt{\\frac{\\sum_{i = 1}^Nx_i^2}{N}} \\tag{9.3} \\end{equation}\\] Sample \\[\\begin{equation} \\bar{x}_Q = \\sqrt{\\frac{\\sum_{i = 1}^nx_i^2}{n}} \\tag{9.4} \\end{equation}\\] The quadratic mean is often used in forestry for reporting average stand diameter (i.e., quadratic mean diameter (QMD)). It is common in most yield tables, simulator output, and stand summaries. 9.2.3 Median The median is defined as the value in an ordered set of values below and above which there is an equal number of values, or which is the arithmetic mean of the two middle values if there is no one middle number. Unlike the arithmetic mean, the median is unaffected by extreme values, making it an attractive alternative to the mean when a data set is highly skewed or has outliers. 9.2.4 Mode The mode is the value that occurs most frequently in a data set. A distribution could have no mode, one mode (unimodal), two modes (bimodal), or several modes (multimodal). The mode is useful for qualitative and quantiative variables. 9.2.5 Symmetrical and Skewed Distributions By looking at our four measures of central tendency for both symmetrical and skewed distributions, we can get an idea of how these measures vary under different circumstances. Figure 9.2 shows a symmetrical distribution. A nice property of symmetrical distriutions is that the mean and median coincide. FIGURE 9.2: Measures of central tendency in a symmetric distribution. Looking at Figures 9.3 and 9.4 this is clearly not the case. In a positive skewed distribution, the distribution is skewed to the right (i.e., positive skew). The arithmetic mean is larger than the median, while the quadratic mean is larger than the arithmetic mean. On the other hand, a negative skewed distribution is skewed to the left (i.e., negative skew). In this case, the arithmetic mean is smaller than the median, while the quadratic mean is larger than the arithmetic mean. Note how in both types of skewness, the quadratic mean is larger than the arithmetic mean. FIGURE 9.3: Measures of central tendency for a positive skewed distribution FIGURE 9.4: Measures of central tendency for a left skewed distribution 9.3 Measures of Dispersion Note that up until this point, we have only considered describing a data set by a single measure of central tendency. While useful, a single measure of central tendency, a type of Point Estimate, does not provide us with any information regarding the variability or dispersion of a data set. Understanding the dispersion of a set of data is arguably equally important as understanding the central tendencies of the data as well. To motivate our exploration of measures of dispersion, consider the following two forest stands shown in Figures 9.5 and 9.6. Suppose the two forest stands had the same mean DBH. If we were to solely focus on this mean value, we would not recognize any differences in DBH between the two stands. However, there are clear differences when looking at the images. The trees in 9.5 all appear to have very similar DBH values. In other words, there is small dispersion around the mean. On the other hand, the trees in 9.6 have a wide range of DBH values; there is a large dispersion around the mean. FIGURE 9.5: A forest with small variability in DBH across individual trees. FIGURE 9.6: A forest with large variability in DBH across individual trees. Thus we see that when reporting values it will be important for us to include measures of central tendency as well as measures of variability around this measure. Here we will briefly explore five measures of dispersion: Variance Standard Deviation Coefficient of Variation Range Inter-quartile Range 9.3.1 Variance One of the most common measures of dispersion is Variance. The variance is a measure of variation of individual values around their mean. Similarly to our measures of central tendency, we distinguish between the population variance and the sample variance: Population: \\[ \\sigma^2 = \\frac{\\sum_{i = 1}^N(x_i - \\mu)^2}{N} \\] where \\(\\mu\\) is the population mean and \\(N\\) is the total number of units in the population. Sample: \\[ s^2 = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})^2}{n - 1} \\] where \\(\\bar{x}\\) is the sample mean and \\(n\\) is the number of sample units. The numerator \\(\\sum_{i = 1}^n(x_i - \\bar{x})^2\\) is an important value known as the Sum of Squares. 9.3.2 Standard Deviation The standard deviation is simply the square root of the variance, and thus also represents a measure of variation of individual values about their mean: Population \\[ \\sigma = \\sqrt{\\sigma^2} \\] Sample \\[ s = \\sqrt{s^2} \\] 9.3.3 Coefficient of Variation The coefficient of variation is the standard deviation expressed as a percentage of the mean. We will primarily use the sample coefficient of variation: \\[ \\text{C.V.} = \\frac{s}{\\bar{x}} \\times 100 \\] 9.4 Bias, Accuracy, and Precision 9.5 Sumarizing Uncertainty A Point Estimate is a single value given as an estimate for a population parameter. They are typically measures of a sample’s central tendency (or derived from such estimates, like sample totals or population totals). As we have noted earlier in our discussion on dispersion estimates, a point estimate is not that useful if we don’t provide a measure of how certain (or uncertain) we are in that value. In fact, you should always be very skeptical if you are reading a publication or any article that provides a point estimate without any uncertainty estimation. This idea was summarized quite succinctly by Frank Freese, a statistician with the Forest Service: We have it on good authority that “you can fool all of the people some of the time”. The oldest device for misleading folks is the barefaced lie. A method that is nearly as effective and far more subtle is to report a sample estimate without any indication of its reliability. Statistics is the science of uncertainty, and so we will always seek to obtain point estimates for our data at hand and provide the associated measures of uncertainty regarding those point estimates. To do this requires the use of some of the measures of dispersion we previously introduced. 9.5.1 Standard Error of the Mean Recall the standard deviation \\(s\\) is a relatively intuitive measure of variation of individual sample observations about their mean (where \\(s = \\sqrt{s^2}\\) and \\(s^2\\) is the sample variance). Now, suppose we draw \\(m\\) different samples of size \\(n\\) from a population, and we then calculate the mean for each of the \\(m\\) different samples (i.e., we have \\(m\\) different means). The Standard Error is a measure of variation of sample means about their mean (i.e., a measure of the variation of the \\(m\\) different means around the mean of those \\(m\\) different means). It is perhaps easiest to think of the standard error as the “standard deviation” among the means of \\(m\\) samples, where \\(m\\) is large. The standard error is a very useful measure that you will likely encounter on a daily basis. In this text, we use the standard error of the mean for two things: Computing confidence intervals for the population parameters (e.g, mean, total). In this case, it allows us to answer the question “how certain are we in our point estimate”. Determining the sample size required to achieve a specified level of certainty in a population parameter estimate. In this case, it allows us to answer the question “how many samples should we collect?”. Clearly these are two questions we want to know the answers to. We will first focus on the first question. But before we tackle that question, we need to define the standard error of the mean so we can calculate it. Given one simple random sample of size \\(n\\) from our population of interest, the standard error of the mean is calculated as: \\[\\begin{equation} s_{\\bar{x}} = \\sqrt{\\frac{s^2}{n}} = \\frac{s}{\\sqrt{n}} \\end{equation}\\] 9.5.2 Confidence Intervals "]
]
